<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <link href="css/vlar.css" type="text/css" rel="stylesheet">

    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    #vlar-contents{text-align:center; padding:10px 0px 10px 0px; margin-top:0px;}
    .vlar-cont-item{width:800px; margin:auto; text-align:left;}
    .list-item{margin-left: 60px; margin-bottom: 40px;}
    .vlar-cont-item-t{ text-align:left; font-size:24px; font-weight:bold; margin-top:20px; margin-bottom:20px; line-height:24px; color:#000000; font-family:Bower,Georgia, "Times New Roman", Times, serif}

    </style>

    <title>Research | vLAR Group</title>

    <link rel="icon" type="image/jpg" href="./imgs/polyu_icon.png">
</head>

<body>
    <div id="vlar-top">
        <div id="vlar-top-con">
            <div id="vlar-top-logo"><img src="imgs/logo.png" />  </div>
            <div id="vlar-top-menus">
                <ul class="vlar-top-menu-u">
                    <li><a href="index.html">Home</a></li>
                    <li class="focusLi">Research</li>
                    <li><a href="people.html">People</a></li>
                </ul>
            </div>
            <div class="vlar-clear"></div>
        </div>
    </div>

    <div id="vlar-contents">
	<div class="vlar-cont-item">
        <div class="vlar-cont-item-t">Research Themes</div>
            <tbody>
            <tr>
                <td width="80%" valign="top">
                    <ul class="list-item" width="80%" valign="top">
                        <li><strong>ML</strong>: unsupervised learning, disentangled representation learning, zero-shot learning, etc.</li>
                    </ul>
                    <ul class="list-item" width="80%" valign="top">
                        <li><strong>CV</strong>: 3D reconstruction, 3D semantic/instance segmentation, neural rendering, etc.</li>
                    </ul>
                    <ul class="list-item" width="80%" valign="top">
                        <li><strong>Robotics</strong>: interaction with 3D scenes, autonomous navigation, path planning, etc.</li>
                    </ul>
                </td>
            </tr>

            <div class="vlar-cont-item-t">Research Papers</div>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                <tbody><tr>
                <td width="20%"><img src="./papers/21_iccv_grf.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                         <p><a href="http://arxiv.org/abs/2010.04595">
                         <papertitle>GRF: Learning a General Radiance Field for 3D Representation and Rendering</papertitle></a>
                         <br>A. Trevithick, <strong>B. Yang</strong> <br>
                         <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2021
                         <br>
                         <a href="http://arxiv.org/abs/2010.04595">arXiv</a> /
                         <a href="https://github.com/alextrevithick/GRF"><font color="red">Code</font></a>
                         <iframe src="https://ghbtns.com/github-btn.html?user=alextrevithick&repo=GRF&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                         <p align="justify" style="font-size:13px">We introduce a simple implicit neural function to represent complex 3D geometries purely from 2D images.
                         </p>
                        <p></p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                <tbody><tr>
                <td width="20%"><img src="./papers/21_tpami_randla.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="http://arxiv.org/abs/2010.04595">
                        <papertitle>Learning Semantic Segmentation of Large-Scale Point Clouds with Random Sampling</papertitle></a>
                        <br>Q. Hu, <strong>B. Yang*</strong>, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham<br>
                        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021 <font color="red"><strong>(IF=16.39)</strong></font><br>
                        <a href="https://arxiv.org/abs/2107.02389">arXiv</a>/
                        <a href="https://ieeexplore.ieee.org/document/9440696">IEEE Xplore</a>/
                        <a href="https://github.com/QingyongHu/RandLA-Net"><font color="red">Code</font></a>
                        <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                        frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                        <br>(* indicates corresponding author)
                        <p align="justify" style="font-size:13px">The journal version of our RandLA-Net. More experiments and analysis are included.</p>
                        </p>
                        <p></p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                <tbody><tr>
                <td width="20%"><img src="./papers/21_arXiv_sqn.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2104.04891">
                        <papertitle>SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds with 1000x Fewer Labels</papertitle></a>
                        <br>Q. Hu, <strong>B. Yang*</strong>, G. Fang, Y. Guo, A. Leonardis, N. Trigoni, A. Markham<br>
                        <em>arXiv</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2104.04891">arXiv</a> /
                        <a href="https://github.com/QingyongHu/SQN"><font color="red">Code</font></a>
                        <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SQN&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        <br>(* indicates corresponding author)
                        <p align="justify" style="font-size:13px">We introduce a simple weakly-supervised neural network to learn precise 3D semantics for large-scale point clouds.
                        </p></p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                <tbody><tr>
                <td width="20%"><img src="./papers/21_cvpr_spinnet.png" alt="PontTuset" width="180" style="border-style: none"></td>
                <td width="80%" valign="top">
                    <p><a href="https://arxiv.org/abs/2011.12149">
                    <papertitle>SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration</papertitle></a>
                    <br>S. Ao*, Q. Hu*, <strong>B. Yang</strong>, A. Markham, Y. Guo<br>
                    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                    <br>
                    <a href="https://arxiv.org/abs/2011.12149">arXiv</a> /
                    <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                    <br>(* indicates equal contributions)
                    <p align="justify" style="font-size:13px">We introduce a simple and general neural network to register pieces of 3D point clouds.
                    </p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/21_cvpr_sensaturban.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                    <p><a href="http://arxiv.org/abs/2009.03137">
                    <papertitle>Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges</papertitle></a>
                    <br>Q. Hu, <strong>B. Yang*</strong>, S. Khalid, W. Xiao, N. Trigoni, A. Markham<br>
                    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                    <br>
                        <!--<font color="red"><strong>..</strong></font><br>-->
                    <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                    <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                    <a href="https://github.com/QingyongHu/SensatUrban"><font color="red">Project page</font></a>
                    <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                    <br>(* indicates corresponding author)
                    <p align="justify" style="font-size:13px">We introduce an urban-scale photogrammetric point cloud dataset
                                and extensively evaluate and analyze the state-of-the-art algorithms on the dataset.</p>
                </td>
                </tr>


                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/21_icra_radarloc.png" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="#">
                        <papertitle>RadarLoc: Learning to Relocalize in FMCW Radar</papertitle></a>
                        <br>W. Wang, P.P.B. de Gusmao, <strong>B. Yang</strong>, A. Markham, N. Trigoni<br>
                        <em>IEEE International Conference on Robotics and Automation (ICRA) </em>, 2021
                        <br>
                        <a href="#">arXiv</a>
                        <p align="justify" style="font-size:13px">We introduce a simple end-to-end neural network with self-attention to estimate global poses from FMCW radar scans.
                        </p>
                       <p></p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/20_arXiv_pointloc.png" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2003.02392">
                        <papertitle>PointLoc: Deep Pose Regressor for LiDAR Point Cloud Localization</papertitle></a>
                        <br>Wei Wang, Bing Wang, Peijun Zhao, Changhao Chen, Ronald Clark, <strong>B. Yang</strong>, Andrew Markham, Niki Trigoni
                        <br>
                        <a href="https://arxiv.org/abs/2003.02392">arXiv, 2020</a>
                        </p><p></p>
                        <p align="justify" style="font-size:13px">We present a learning-based LiDAR relocalization framework to efficiently estimate 6-DoF poses from LiDAR point clouds. </p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/20_cvpr_randlanet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/1911.11236">
                        <papertitle>RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</papertitle></a>
                        <br>Q. Hu, <strong>B. Yang*</strong>, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                        <br>
                        <a href="https://arxiv.org/abs/1911.11236">arXiv</a> /
                        <a href="http://www.semantic3d.net/view_results.php">Semantic3D Benchmark</a> /
                        <font color="red"> News:</font>
                        <a href="https://mp.weixin.qq.com/s/k_oROm1Zr6l0YNKGELx3Bw"><font color="red">(新智元,</font></a>
                        <a href="https://mp.weixin.qq.com/s/Ed9v6I6l2tLTHmMW7B3O3g"><font color="red">AI科技评论,</font></a>
                        <a href="https://mp.weixin.qq.com/s/TTv6pSPjmdsEF4kvVY-ZzQ"><font color="red">CVer)</font>/</a>
                        <a href="https://www.youtube.com/watch?v=Ar3eY_lwzMk"><font color="red">Video</font></a>/
                        <a href="https://github.com/QingyongHu/RandLA-Net"><font color="red">Code</font></a>
                        <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                         frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                        <br>(* indicates corresponding author)
                        </p><p></p>
                        <p align="justify" style="font-size:13px">We introduce an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. </p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/19_neurips_3d_bonet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/1906.01140">
                        <papertitle>Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</papertitle></a>
                        <br><strong>B. Yang</strong>, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham, N. Trigoni
                        <br>
                        <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019 <font color="red"><strong>(Spotlight, 200/6743)</strong></font>
                        <br>
                        <!--<font color="red"><strong>..</strong></font><br>-->
                        <a href="https://arxiv.org/abs/1906.01140">arXiv</a> /
                        <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> /
                        <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> /
                        <font color="red"> News:</font>
                        <a href="https://mp.weixin.qq.com/s/jHbWf_SSZE_J6NRJR-96sQ"><font color="red">(新智元,</font></a>
                        <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                        <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                        <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                        <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a>
                        <a href="https://mp.weixin.qq.com/s/gybhVw3D4ykAHsVGzazWNw"><font color="red">泡泡机器人)</font>/</a>
                        <a href="https://www.youtube.com/watch?v=Bk727Ec10Ao"><font color="red">Video</font></a>/
                        <a href="https://github.com/Yang7879/3D-BoNet"><font color="red">Code</font></a>
                        <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-BoNet&type=star&count=true&size=small"
                        frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                        <p align="justify" style="font-size:13px">We propose a simple and efficient neural architecture for accurate 3D instance segmentation on point clouds.
                         It achieves the SOTA performance on ScanNet and S3DIS (June 2019).</p>
                       <p></p>
                   </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/19_iros_deeppco.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/1910.11088">
                        <papertitle>DeepPCO: End-to-End Point Cloud Odometry through Deep Parallel Neural Network</papertitle></a>
                        <br>W. Wang,  M.R.U. Saputra, P. Zhao, P. Gusmao, <strong>B. Yang</strong>, C. Chen, A. Markham, N. Trigoni<br>
                        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2019 <br>
                        <a href="https://arxiv.org/abs/1910.11088">arXiv</a>
                        </p><p></p>
                        <p align="justify" style="font-size:13px">We propose a novel end-to-end deep parallel neural network to estimate the 6-DOF poses using consecutive 3D point clouds.</p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/19_ijcv_attsets.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://link.springer.com/article/10.1007/s11263-019-01217-w">
                        <papertitle>Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction</papertitle></a>
                        <br><strong>B. Yang</strong>, S. Wang, A. Markham, N. Trigoni<br>
                        <em>International Journal of Computer Vision (IJCV)</em>, 2019 <font color="red"><strong>(IF=6.07)</strong></font><br>
                        <a href="https://arxiv.org/abs/1808.00758">arXiv</a>/
                        <a href="https://link.springer.com/article/10.1007/s11263-019-01217-w">Springer Open Access</a>/
                        <a href="https://github.com/Yang7879/AttSets"><font color="red">Code</font></a>
                        <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=AttSets&type=star&count=true&size=small"
                        frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                        </p><p></p>
                        <p align="justify" style="font-size:13px"> We propose an attentive aggregation module together
                            with a training algorithm for multi-view 3D object reconstruction.
                            It outperforms all existing poolings and recurrent neural networks.</p>
                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="20" cellpadding="20">
                    <tbody><tr>
                    <td width="20%"><img src="./papers/19_cvprw_embeddings.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.html">
                        <papertitle>Learning Semantically Meaningful Embeddings Using Linear Constraints</papertitle></a>
                        <br>S. Lin, <strong>B. Yang</strong>, R. Birke, R. Clark<br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)</em>, 2019<br>
                        <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.html">CVF Open Access</a>
                        </p><p></p>
                        <p align="justify" style="font-size:13px">We propose a simple embedding learning method that jointly optimises for an auto-encoding reconstruction task
                            and for estimating the corresponding attribute labels.</p>
                    </td>
                </tr>
            </tbody>
            </table>

            </td>
            </tr>
        </tbody>
        </table>

    </div>
    </div>
</body>
</html>
